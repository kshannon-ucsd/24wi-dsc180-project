{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset= pd.read_csv('second_model_subset.csv')\n",
    "features = pd.read_csv('second_models_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'chest-xrays/'\n",
    "segmented = pd.read_csv(path+\"CXLSeg-segmented.csv\")\n",
    "xray = pd.read_csv(path+'CXLSeg-metadata.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_format(date):\n",
    "    date = str(date)\n",
    "    formatted_date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "    return formatted_date\n",
    "\n",
    "def time_format(time):\n",
    "    time = str(time)\n",
    "    time = time.split(\".\")[0]\n",
    "    while len(time) != 6:\n",
    "        time = \"0\" + time\n",
    "    formatted_time = f\"{time[:2]}:{time[2:4]}:{time[4:6]}\"\n",
    "    return formatted_time\n",
    "\n",
    "def convert_datetime(input_date):\n",
    "    return datetime.fromisoformat(input_date)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['suspected_infection_time'].iloc[32971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[\"admittime\"] = pd.to_datetime(subset[\"admittime\"])\n",
    "subset[\"dischtime\"] = pd.to_datetime(subset[\"dischtime\"])\n",
    "subset['suspected_infection_time'] = pd.to_datetime(subset['suspected_infection_time'])\n",
    "\n",
    "xray = xray.assign(formatted_date = xray[\"StudyDate\"].apply(date_format))\n",
    "xray = xray.assign(formatted_time = xray[\"StudyTime\"].apply(time_format))\n",
    "xray = xray.assign(studytime = (xray[\"formatted_date\"] + \" \" + xray[\"formatted_time\"]).apply(convert_datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['suspected_infection_time'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subset = subset[\\\n",
    "    (((subset['suspected_infection_time'].dt.normalize()-subset['admittime'].dt.normalize()).dt.days)>=0)\\\n",
    "    | (((subset['suspected_infection_time'].dt.normalize()-subset['admittime'].dt.normalize()).dt.days).isna())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['days'] = (subset['suspected_infection_time'].dt.normalize()-subset['admittime'].dt.normalize()).dt.days\n",
    "subset['days'] = subset['days'].fillna(-1)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting xray dataset to make merge more efficient\n",
    "xray_merge = xray[[\"subject_id\", \"study_id\", \"ViewPosition\", \"studytime\"]]\n",
    "# First merge\n",
    "merging = subset.merge(xray_merge, left_on = \"subject_id\", right_on = \"subject_id\")\n",
    "# Matching each xray to hospital admission\n",
    "matched_dates = merging[(merging[\"studytime\"] >= merging[\"admittime\"]) & (merging[\"studytime\"] <= merging[\"dischtime\"])].reset_index(drop = True)\n",
    "# Preprocessing segmented for merging\n",
    "segmented_merged = segmented[[\"subject_id\", \"study_id\", \"dicom_id\", \"DicomPath\", \"No Finding\"]]\n",
    "segmented_merged[\"No Finding\"] = segmented_merged[\"No Finding\"].fillna(-1)\n",
    "segmented_merged[\"Abnormal\"] = (segmented_merged[\"No Finding\"] * -1)\n",
    "segmented_merged = segmented_merged.drop(columns = [\"No Finding\"])\n",
    "# Final merge\n",
    "complete_merged = matched_dates.merge(segmented_merged, on = [\"subject_id\", \"study_id\"])[[\"subject_id\", \"hadm_id\", \"stay_id\", \"study_id\", \n",
    "                                                                       \"admittime\", \"dischtime\", \"days\", \"studytime\", \"ViewPosition\",\n",
    "                                                                       \"dicom_id\", \"DicomPath\", \"Abnormal\", \"los\", \n",
    "                                                                       \"chronic_pulmonary_disease\", \"sepsis3\"]]\n",
    "\n",
    "complete_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('second_models_features.csv')\n",
    "features = features[features['subject_id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = features[features['subject_id']==16192578.0]\n",
    "# sub.sort_values(by = 'charttime', ascending = False)\n",
    "\n",
    "# sub.merge(sub.groupby(['subject_id', 'hadm_id', 'stay_id'])[['charttime']].max().reset_index(), on = 'charttime').columns\n",
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recents = features.groupby(['subject_id', 'hadm_id', 'stay_id'])[['charttime']].max().reset_index()\n",
    "# features.merge(recents, on = ['subject_id', 'hadm_id', 'stay_id', 'charttime'])\n",
    "recents = features.sort_values(['subject_id', 'hadm_id', 'stay_id', 'charttime']).groupby(['subject_id', 'hadm_id', 'stay_id']).tail(1)\n",
    "# recents\n",
    "recents = recents.reset_index().drop(columns = 'index')\n",
    "recents.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = features.groupby(['subject_id', 'hadm_id', 'stay_id'])[['heart_rate', 'sbp',\n",
    "       'sbp_ni', 'mbp', 'mbp_ni', 'resp_rate', 'temperature', 'platelet',\n",
    "       'wbc', 'bands', 'lactate', 'inr', 'ptt', 'creatinine', 'bilirubin']].mean().reset_index()\n",
    "means.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_squeeze = recents.combine_first(means)\n",
    "feat_squeeze.notna().sum()/(feat_squeeze.isna().sum()+feat_squeeze.notna().sum())\n",
    "\n",
    "\n",
    "feat_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = complete_merged.merge(feat_squeeze, how = 'left', on = ['subject_id', 'hadm_id', 'stay_id'])\n",
    "full_data = full_data.drop_duplicates('dicom_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.columns\n",
    "feats = ['Abnormal', 'bilirubin', 'creatinine', 'heart_rate', 'inr', 'mbp', 'platelet',\n",
    "       'ptt', 'resp_rate', 'sbp', 'wbc', 'days']\n",
    "\n",
    "X = full_data[feats].dropna().drop(columns = 'days')\n",
    "y = full_data[feats].dropna()['days'].apply(lambda x: '3+' if x > 3 else str(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Normalize or standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert target labels to binary format (multilabel)\n",
    "y = pd.get_dummies(full_data[feats].dropna()['days'].apply(lambda x: '3+' if x > 3 else str(x)))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the multilabel logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "multi_target_model = MultiOutputClassifier(log_reg)\n",
    "multi_target_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = multi_target_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model (example: accuracy score for each label)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data.columns\n",
    "# full_data[['subject_id', 'hadm_id', 'stay_id_x', 'study_id', 'admittime',\n",
    "#        'dischtime', 'studytime', 'ViewPosition', 'dicom_id', 'charttime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[sample['stay_id_x']==30000646]\n",
    "\n",
    "# sample[sample['stay_id_x']==30000646]['study_id'].value_counts()\n",
    "\n",
    "# (sample[sample['study_id']==55490538]['charttime']-sample[sample['study_id']==55490538]['studytime']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample['days'] = abs(sample['studytime']-sample['charttime'])\n",
    "\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category, group in sample.groupby(['hadm_id', 'study_id']):\n",
    "#     print(f\"Category: {category}\")\n",
    "#     print(group, \"\\n\")\n",
    "\n",
    "# sample.merge(sample.groupby(['hadm_id', 'study_id'])[['days']].reset_index(), on = 'days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.groupby(['hadm_id', 'study_id'])[['days']].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
