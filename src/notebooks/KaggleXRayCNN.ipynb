{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823746dd-43d3-44f4-8edc-dad95b3205db",
   "metadata": {},
   "source": [
    "# DSC 180B CNN Notebook (Kaggle Dataset)\n",
    "\n",
    "### Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf024e3-b5d6-4638-ad04-2309bec88f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting opencv-python-headless\n",
      "  Using cached opencv-python-headless-4.11.0.86.tar.gz (95.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from opencv-python-headless) (1.24.3)\n",
      "Building wheels for collected packages: opencv-python-headless\n",
      "  Building wheel for opencv-python-headless (pyproject.toml) ... \u001b[?25l\\^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting kaggle\n",
      "  Obtaining dependency information for kaggle from https://files.pythonhosted.org/packages/83/a9/3208f2007dd57d47329f766b3813bb61975170a7ed52627a338566a6c490/kaggle-1.7.4-py3-none-any.whl.metadata\n",
      "  Using cached kaggle-1.7.4-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: bleach in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (3.4)\n",
      "Requirement already satisfied: protobuf in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (68.0.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: webencodings in /Users/rohan/anaconda3/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/rohan/anaconda3/lib/python3.11/site-packages (from bleach->kaggle) (24.1)\n",
      "Using cached kaggle-1.7.4-py3-none-any.whl (173 kB)\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python-headless -y \n",
    "!pip uninstall opencv-python -y\n",
    "!pip uninstall opencv-contrib-python -y\n",
    "!pip install opencv-python-headless\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d8ca88-417b-44d9-9ff0-562d525a2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948106a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for storing images\n",
    "local_raw_download_path = '../../data/raw/'\n",
    "local_processed_download_path = '../../data/processed/'\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'test'), exist_ok=True)\n",
    "\n",
    "os.makedirs(local_processed_download_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'test'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3dac93-2f18-48b4-8984-60b7cae45e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol message DatasetInfo has no \"info\" field.\n",
      "Dataset downloaded successfully!\n",
      "Zipped dataset not found!\n"
     ]
    }
   ],
   "source": [
    "def setup_kaggle(username: str, key: str):\n",
    "    \"\"\"\n",
    "    Configures Kaggle API access and downloads the Chest X-ray Pneumonia dataset.\n",
    "    :param username: Kaggle username\n",
    "    :param key: Kaggle API key\n",
    "    \"\"\"\n",
    "    # Define the kaggle credentials path\n",
    "    kaggle_dir = Path.home() / \".kaggle\"  # Use the user's home directory\n",
    "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
    "    \n",
    "    # Write Kaggle credentials\n",
    "    kaggle_credentials = {\"username\": username, \"key\": key}\n",
    "    with open(kaggle_json_path, \"w\") as f:\n",
    "        json.dump(kaggle_credentials, f)\n",
    "    \n",
    "    # Set permissions for security\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "    \n",
    "    # Ensure kaggle CLI is in the path\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = str(kaggle_dir)\n",
    "        \n",
    "    # Download the dataset\n",
    "    dataset_path = Path(\"chest_xray_pneumonia\")\n",
    "    dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "    os.system(f\"kaggle datasets download -d paultimothymooney/chest-xray-pneumonia -p {dataset_path} --unzip\")\n",
    "    \n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "    \n",
    "    # Define the unpack path\n",
    "    unpack_path = Path(\"../../data/raw\")\n",
    "    unpack_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Unpack the zipped dataset into the specified path\n",
    "    zip_file = dataset_path / \"chest-xray-pneumonia.zip\"\n",
    "    \n",
    "    if zip_file.exists():\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unpack_path)\n",
    "        print(f\"Dataset unpacked successfully to {unpack_path}\")\n",
    "        \n",
    "        # Delete the zip file after unpacking\n",
    "        zip_file.unlink()\n",
    "        print(\"Zip file deleted after unpacking.\")\n",
    "    else:\n",
    "        print(\"Zipped dataset not found!\")\n",
    "\n",
    "# Get user input for Kaggle credentials\n",
    "username = input(\"Enter your Kaggle Username: \")\n",
    "key = input(\"Enter your generated Kaggle API key: \")\n",
    "\n",
    "setup_kaggle(username, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04456901-0b00-4503-89ee-6f56a3668a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img, img_size=256, crop_size=224, is_train=True):\n",
    "    # Resize and crop\n",
    "    img = tf.image.resize(img, (img_size, img_size))\n",
    "    img = tf.image.central_crop(img, crop_size / img_size)\n",
    "\n",
    "    if is_train:\n",
    "        # Random horizontal flip\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "        # Random brightness and contrast\n",
    "        img = tf.image.random_brightness(img, max_delta=0.25)\n",
    "        img = tf.image.random_contrast(img, lower=0.75, upper=1.25)\n",
    "    \n",
    "        # Random affine transformation (rotation & translation)\n",
    "        def random_affine(img):\n",
    "            transform = transforms.RandomAffine(\n",
    "                degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)\n",
    "            )\n",
    "            img = img.numpy().astype(np.uint8)  # Ensure the image is in uint8 format\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_pil = transform(img_pil)  # Apply affine transform\n",
    "            img = np.array(img_pil).astype(np.float32)  # Convert back to float32\n",
    "            return img\n",
    "        \n",
    "        img = random_affine(img)\n",
    "        \n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = {'filename': [], 'label': []}\n",
    "val_df = {'filename': [], 'label': []}\n",
    "test_df = {'filename': [], 'label': []}\n",
    "\n",
    "def extract_label_from_filename(filename):\n",
    "    \"\"\"Extracts label from filename based on known keywords.\"\"\"\n",
    "    if \"bacteria\" in filename:\n",
    "        return \"bacteria\"\n",
    "    elif \"virus\" in filename:\n",
    "        return \"virus\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "for folder_prefix, dataset in zip([\"train/\", \"val/\", \"test/\"], [train_df, val_df, test_df]):\n",
    "    for file in os.listdir(os.path.join(\"../../data/raw/\", folder_prefix)):\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        label = extract_label_from_filename(filename)\n",
    "        dataset['filename'].append(filename)\n",
    "        dataset['label'].append(label)\n",
    "\n",
    "        # Define local paths for raw and processed data\n",
    "        processed_local_folder = os.path.join(\"../../data/processed\", folder_prefix)\n",
    "\n",
    "        # Create the directories if they don't exist\n",
    "        os.makedirs(processed_local_folder, exist_ok=True)\n",
    "\n",
    "        # Load and process the image\n",
    "        img = tf.io.read_file(file)\n",
    "        \n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {file}\")\n",
    "            continue\n",
    "\n",
    "        is_train = (folder_prefix == \"train/\")\n",
    "            \n",
    "        img = preprocess_img(img, 256, 224, is_train)\n",
    "\n",
    "        img = img.numpy() if isinstance(img, tf.Tensor) else img\n",
    "\n",
    "        # Save processed image\n",
    "        processed_image_path = os.path.join(processed_local_folder, filename)\n",
    "        cv2.imwrite(processed_image_path, img)\n",
    "                    \n",
    "print(\"Completed fetching all objects.\")\n",
    "print(f\"Train: {len(train_df['filename'])} images\")\n",
    "print(f\"Validation: {len(val_df['filename'])} images\")\n",
    "print(f\"Test: {len(test_df['filename'])} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_df = pd.DataFrame(train_df)\n",
    "val_df = pd.DataFrame(val_df)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to binary labels\n",
    "train_df['binary_label'] = train_df['label'].apply(lambda x: '0' if x == 'normal' else '1')\n",
    "val_df['binary_label'] = val_df['label'].apply(lambda x: '0' if x == 'normal' else '1')\n",
    "test_df['binary_label'] = test_df['label'].apply(lambda x: '0' if x == 'normal' else '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, directory='../../data/processed/train', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df, directory='../../data/processed/val', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df, directory='../../data/processed/test', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce593e",
   "metadata": {},
   "source": [
    "### Healthy vs Unhealthy Lung Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "healthy_x_rays = train_df[train_df['binary_label'] == '0'][:10]\n",
    "abnormal_x_rays = train_df[train_df['binary_label'] == '1'][:10]\n",
    "\n",
    "raw_paths = \"../../data/raw/train/\"\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Load the healthy X-ray image\n",
    "    healthy_image_path = raw_paths + healthy_x_rays.iloc[i]['filename']\n",
    "    healthy_img = mpimg.imread(healthy_image_path)\n",
    "    ax[0].imshow(healthy_img)\n",
    "    ax[0].set_title('Healthy X-Ray')\n",
    "\n",
    "    # Load the abnormal X-ray image\n",
    "    abnormal_image_path = raw_paths + abnormal_x_rays.iloc[i]['filename']\n",
    "    abnormal_img = mpimg.imread(abnormal_image_path)\n",
    "    ax[1].imshow(abnormal_img)\n",
    "    ax[1].set_title('Abnormal X-Ray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "# Load ResNet50V2\n",
    "resnet50 = ResNet50V2(include_top=False, weights='imagenet', pooling='max', input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    resnet50,\n",
    "    Dense(512, kernel_regularizer=l2(5e-4)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.6),  \n",
    "    Dense(256, kernel_regularizer=l2(5e-4)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, kernel_regularizer=l2(5e-4)),\n",
    "    Activation('relu'),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Implement Cosine Annealing\n",
    "initial_lr = 0.0001  # Start higher to encourage exploration\n",
    "first_decay_steps = 3 * 163\n",
    "\n",
    "cosine_decay_restarts = CosineDecayRestarts(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    first_decay_steps=first_decay_steps,\n",
    "    t_mul=2.0,  # Double cycle length after each restart\n",
    "    m_mul=0.8,  # Reduce LR after each restart\n",
    "    alpha=1e-6  # Small floor LR to prevent zero updates\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=cosine_decay_restarts)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "checkpoint_filepath = 'final_model_1.keras'\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_filepath, \n",
    "                                   monitor='val_loss', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
