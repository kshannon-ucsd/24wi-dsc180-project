{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O-NTuNnnw0z"
   },
   "source": [
    "# DSC 180B CNN Notebook\n",
    "\n",
    "### Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python-headless -y \n",
    "!pip uninstall opencv-python -y\n",
    "!pip uninstall opencv-contrib-python -y\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMtca1ORnw00"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from PIL import Image\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXTd1erynw01"
   },
   "outputs": [],
   "source": [
    "def preprocess_img(img, img_size=256, crop_size=224, is_train=True):\n",
    "    # Resize and crop\n",
    "    img = tf.image.resize(img, (img_size, img_size))\n",
    "    img = tf.image.central_crop(img, crop_size / img_size)\n",
    "\n",
    "    if is_train:\n",
    "        # Random horizontal flip\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "        # Random brightness and contrast\n",
    "        img = tf.image.random_brightness(img, max_delta=0.25)\n",
    "        img = tf.image.random_contrast(img, lower=0.75, upper=1.25)\n",
    "    \n",
    "        # Random affine transformation (rotation & translation)\n",
    "        def random_affine(img):\n",
    "            transform = transforms.RandomAffine(\n",
    "                degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)\n",
    "            )\n",
    "            img = img.numpy().astype(np.uint8)  # Ensure the image is in uint8 format\n",
    "            img_pil = Image.fromarray(img)\n",
    "            img_pil = transform(img_pil)  # Apply affine transform\n",
    "            img = np.array(img_pil).astype(np.float32)  # Convert back to float32\n",
    "            return img\n",
    "        \n",
    "        img = random_affine(img)\n",
    "        \n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for storing images\n",
    "local_raw_download_path = '../../data/raw/'\n",
    "local_processed_download_path = '../../data/processed/'\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_raw_download_path, 'test'), exist_ok=True)\n",
    "\n",
    "os.makedirs(local_processed_download_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(local_processed_download_path, 'test'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# Specify your S3 bucket name\n",
    "bucket_name = \"sagemaker-capstone-bucket\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "train_df = {'filename': [], 'label': []}\n",
    "val_df = {'filename': [], 'label': []}\n",
    "test_df = {'filename': [], 'label': []}\n",
    "\n",
    "def extract_label_from_filename(filename):\n",
    "    \"\"\"Extracts label from filename based on known keywords.\"\"\"\n",
    "    if \"bacteria\" in filename:\n",
    "        return \"bacteria\"\n",
    "    elif \"virus\" in filename:\n",
    "        return \"virus\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "try:\n",
    "    print(f\"Connecting to bucket: {bucket_name}\")\n",
    "\n",
    "    # Use a continuation token for pagination\n",
    "    continuation_token = None\n",
    "\n",
    "    for folder_prefix, dataset in zip([\"train/\", \"val/\", \"test/\"], [train_df, val_df, test_df]):\n",
    "        continuation_token = None\n",
    "        while True:\n",
    "            # List objects with pagination\n",
    "            list_params = {\"Bucket\": bucket_name, \"Prefix\": folder_prefix}\n",
    "            if continuation_token:\n",
    "                list_params[\"ContinuationToken\"] = continuation_token\n",
    "    \n",
    "            response = s3.list_objects_v2(**list_params)\n",
    "    \n",
    "            if \"Contents\" in tqdm(response):\n",
    "                for obj in tqdm(response[\"Contents\"]):\n",
    "                    key = obj[\"Key\"]\n",
    "    \n",
    "                    # Collect .jpg image paths that are not masks\n",
    "                    if key.endswith(\".jpeg\"):\n",
    "                        filename = key.split('/')[-1]\n",
    "                        label = extract_label_from_filename(key)\n",
    "                        dataset['filename'].append(filename)\n",
    "                        dataset['label'].append(label)\n",
    "\n",
    "                        # Define local paths for raw and processed data\n",
    "                        raw_local_folder = os.path.join(\"../../data/raw\", folder_prefix)\n",
    "                        processed_local_folder = os.path.join(\"../../data/processed\", folder_prefix)\n",
    "\n",
    "                        # Create the directories if they don't exist\n",
    "                        os.makedirs(raw_local_folder, exist_ok=True)\n",
    "                        os.makedirs(processed_local_folder, exist_ok=True)\n",
    "                        \n",
    "                        # Construct the full path for the raw file to download\n",
    "                        raw_local_file_path = os.path.join(raw_local_folder, filename)\n",
    "                        try:\n",
    "                            s3.download_file(bucket_name, key, raw_local_file_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error downloading {key}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                        # Load and process the image\n",
    "                        img = tf.io.read_file(raw_local_file_path)\n",
    "                        img = tf.image.decode_jpeg(img, channels=3)\n",
    "                        if img is None:\n",
    "                            print(f\"Failed to load image: {raw_local_file_path}\")\n",
    "                            continue\n",
    "\n",
    "                        is_train = (folder_prefix == \"train/\")\n",
    "                            \n",
    "                        img = preprocess_img(img, 256, 224, is_train)\n",
    "                \n",
    "                        img = img.numpy() if isinstance(img, tf.Tensor) else img\n",
    "                \n",
    "                        # Save processed image\n",
    "                        processed_image_path = os.path.join(processed_local_folder, filename)\n",
    "                        cv2.imwrite(processed_image_path, img)\n",
    "                        \n",
    "            # Check if there's another page of results\n",
    "            if response.get(\"IsTruncated\"):\n",
    "                continuation_token = response[\"NextContinuationToken\"]\n",
    "            else:\n",
    "                break  # No more objects to fetch\n",
    "\n",
    "    print(\"Completed fetching all objects.\")\n",
    "    print(f\"Train: {len(train_df['filename'])} images\")\n",
    "    print(f\"Validation: {len(val_df['filename'])} images\")\n",
    "    print(f\"Test: {len(test_df['filename'])} images\")\n",
    "\n",
    "except NoCredentialsError:\n",
    "    print(\"Error: No AWS credentials found.\")\n",
    "except PartialCredentialsError:\n",
    "    print(\"Error: Incomplete AWS credentials configuration.\")\n",
    "except s3.exceptions.NoSuchBucket:\n",
    "    print(f\"Error: Bucket {bucket_name} does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_df = pd.DataFrame(train_df)\n",
    "val_df = pd.DataFrame(val_df)\n",
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to binary labels\n",
    "train_df['binary_label'] = train_df['label'].apply(lambda x: '0' if x == 'normal' else '1')\n",
    "val_df['binary_label'] = val_df['label'].apply(lambda x: '0' if x == 'normal' else '1')\n",
    "test_df['binary_label'] = test_df['label'].apply(lambda x: '0' if x == 'normal' else '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, directory='../../data/processed/train', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df, directory='../../data/processed/val', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df, directory='../../data/processed/test', x_col='filename', y_col='binary_label', target_size=(224, 224), batch_size=32, class_mode='binary', validate_filenames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtvKCB_0nw02"
   },
   "source": [
    "### Healthy vs Unhealthy Lung Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gWaewoErnw02",
    "outputId": "4b222205-90b4-408c-fded-ea8f2f8240ae"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "healthy_x_rays = train_df[train_df['binary_label'] == '0'][:10]\n",
    "abnormal_x_rays = train_df[train_df['binary_label'] == '1'][:10]\n",
    "\n",
    "raw_paths = \"../../data/raw/train/\"\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Load the healthy X-ray image\n",
    "    healthy_image_path = raw_paths + healthy_x_rays.iloc[i]['filename']\n",
    "    healthy_img = mpimg.imread(healthy_image_path)\n",
    "    ax[0].imshow(healthy_img)\n",
    "    ax[0].set_title('Healthy X-Ray')\n",
    "\n",
    "    # Load the abnormal X-ray image\n",
    "    abnormal_image_path = raw_paths + abnormal_x_rays.iloc[i]['filename']\n",
    "    abnormal_img = mpimg.imread(abnormal_image_path)\n",
    "    ax[1].imshow(abnormal_img)\n",
    "    ax[1].set_title('Abnormal X-Ray')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFjcxOYanw02",
    "outputId": "f3109189-486c-4091-f771-c9cc01f47ffe"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "# Load ResNet50V2\n",
    "resnet50 = ResNet50V2(include_top=False, weights='imagenet', pooling='max', input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    resnet50,\n",
    "    Dense(512, kernel_regularizer=l2(5e-4)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.6),  \n",
    "    Dense(256, kernel_regularizer=l2(5e-4)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, kernel_regularizer=l2(5e-4)),\n",
    "    Activation('relu'),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Implement Cosine Annealing\n",
    "initial_lr = 0.0001  # Start higher to encourage exploration\n",
    "first_decay_steps = 3 * 163\n",
    "\n",
    "cosine_decay_restarts = CosineDecayRestarts(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    first_decay_steps=first_decay_steps,\n",
    "    t_mul=2.0,  # Double cycle length after each restart\n",
    "    m_mul=0.8,  # Reduce LR after each restart\n",
    "    alpha=1e-6  # Small floor LR to prevent zero updates\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=cosine_decay_restarts)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "checkpoint_filepath = 'final_model_1.keras'\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_filepath, \n",
    "                                   monitor='val_loss', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=test_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
