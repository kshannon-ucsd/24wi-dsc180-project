{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O-NTuNnnw0z"
   },
   "source": [
    "# DSC 180B CNN Notebook\n",
    "\n",
    "### Data Loading and Getting the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMtca1ORnw00"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from imgaug import augmenters as iaa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "gdHI_OVHnw00",
    "outputId": "d8e7197e-e4bf-4302-d10f-bde601ffccd7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/sql-data/Data_Entry_2017.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agAfUe7Bnw01"
   },
   "outputs": [],
   "source": [
    "def get_label(row):\n",
    "    if 'No Finding' in row['Finding Labels']:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "u3zK6c4Onw01",
    "outputId": "e8fd395a-6c6a-4c22-c2c8-3d52e0ed314d"
   },
   "outputs": [],
   "source": [
    "df['Finding Labels'] = df.apply(lambda row: get_label(row), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXTd1erynw01"
   },
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img  \n",
    "\n",
    "def random_horizontal_flip(img):\n",
    "    if np.random.rand() < 0.5:\n",
    "        img = cv2.flip(img, 1)\n",
    "    return img\n",
    "\n",
    "def apply_clahe(img):\n",
    "    red_channel = img[:, :, 0]\n",
    "    green_channel = img[:, :, 1]\n",
    "    blue_channel = img[:, :, 2]\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    \n",
    "    red_channel = clahe.apply(red_channel)\n",
    "    green_channel = clahe.apply(green_channel)\n",
    "    blue_channel = clahe.apply(blue_channel)\n",
    "    \n",
    "    return cv2.merge((red_channel, green_channel, blue_channel))\n",
    "\n",
    "def add_random_rotation(img, max_angle=10):\n",
    "    angle = np.random.uniform(-max_angle, max_angle)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1] // 2, img.shape[0] // 2), angle, 1)\n",
    "    return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "\n",
    "def normalize_image(img):\n",
    "    return img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f83MFlhwnw01",
    "outputId": "44ff8092-dc9f-4a9b-bc12-760ec7996498"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# load png images into X and y\n",
    "for i in range(len(df)):\n",
    "    image_name = df['Image Index'][i]\n",
    "    png_path = os.path.join('../../data/raw', image_name) # Use os.path.join to create the path\n",
    "\n",
    "    # Check if the file exists before trying to read it\n",
    "    if not os.path.exists(png_path):\n",
    "        # print(f\"Image file not found: {png_path}\")\n",
    "        continue\n",
    "\n",
    "    # print(\"Image found\")\n",
    "\n",
    "    img = cv2.imread(png_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if img is None:\n",
    "        # print(f\"Failed to load image: {png_path}\")\n",
    "        continue\n",
    "\n",
    "    img = transform_image(img)\n",
    "    img = random_horizontal_flip(img)\n",
    "    img = apply_clahe(img)\n",
    "    img = add_random_rotation(img)\n",
    "    img = normalize_image(img)\n",
    "\n",
    "    X.append(img)\n",
    "    y.append(df['Finding Labels'][i])\n",
    "\n",
    "    # print(len(X))\n",
    "    # print(len(y))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd2qn8QgQuMi"
   },
   "outputs": [],
   "source": [
    "# # Convert to tensor to be fed to ResNet50\n",
    "# X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hchWDWbUQ0Om",
    "outputId": "899b68b6-169f-4d3e-d039-909bbbdb9bb5"
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtvKCB_0nw02"
   },
   "source": [
    "### Healthy vs Unhealthy Lung Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gWaewoErnw02",
    "outputId": "4b222205-90b4-408c-fded-ea8f2f8240ae"
   },
   "outputs": [],
   "source": [
    "healthy_x_rays = []\n",
    "abnormal_x_rays = []\n",
    "i = 0\n",
    "while len(healthy_x_rays) < 10 or len(abnormal_x_rays) < 10:\n",
    "    if y_train[i] == 0:\n",
    "        healthy_x_rays.append(X_train[i])\n",
    "    else:\n",
    "        abnormal_x_rays.append(X_train[i])\n",
    "    i += 1\n",
    "\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].imshow(healthy_x_rays[i], cmap='gray')\n",
    "    ax[0].set_title('Healthy X-Ray')\n",
    "    ax[1].imshow(abnormal_x_rays[i], cmap='gray')\n",
    "    ax[1].set_title('Abnormal X-Ray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFjcxOYanw02",
    "outputId": "f3109189-486c-4091-f771-c9cc01f47ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.1373 - accuracy: 0.5500\n",
      "Epoch 1: val_loss improved from inf to 1.65515, saving model to best_model.keras\n",
      "5/5 [==============================] - 6s 543ms/step - loss: 1.1373 - accuracy: 0.5500 - val_loss: 1.6552 - val_accuracy: 0.3500\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4250 - accuracy: 0.8125\n",
      "Epoch 2: val_loss improved from 1.65515 to 0.87390, saving model to best_model.keras\n",
      "5/5 [==============================] - 1s 287ms/step - loss: 0.4250 - accuracy: 0.8125 - val_loss: 0.8739 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9125\n",
      "Epoch 3: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.2286 - accuracy: 0.9125 - val_loss: 0.9634 - val_accuracy: 0.5500\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9875\n",
      "Epoch 4: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0914 - accuracy: 0.9875 - val_loss: 1.4011 - val_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 5: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 1.5841 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 6: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 1.4183 - val_accuracy: 0.4500\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 7: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.2653 - val_accuracy: 0.4500\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 8: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 1.1907 - val_accuracy: 0.4500\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 9: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.1850 - val_accuracy: 0.4500\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 10: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.2136 - val_accuracy: 0.4500\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 11: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.2613 - val_accuracy: 0.4500\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 2.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.87390\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 1.3009 - val_accuracy: 0.4500\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Load ResNet50V2\n",
    "resnet50 = ResNet50V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Unfreeze deeper layers for fine-tuning\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    resnet50,\n",
    "    Flatten(),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Implement Cosine Annealing\n",
    "initial_lr = 1e-4\n",
    "cosine_decay_restarts = CosineDecayRestarts(initial_learning_rate=initial_lr,\n",
    "                                            first_decay_steps=5000,\n",
    "                                            t_mul=2.0,  # Increases period after each restart\n",
    "                                            m_mul=0.8,  # Reduce max learning rate after each restart\n",
    "                                            alpha=1e-6)\n",
    "\n",
    "# Compile model with Cosine Annealing scheduler\n",
    "model.compile(optimizer=Adam(learning_rate=cosine_decay_restarts),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=100,\n",
    "                    batch_size=16,\n",
    "                    class_weight=class_weights_dict,\n",
    "                    callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HS4RT6_nw02"
   },
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred > 0.5\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# output precision, recall, f1-score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqcnoIunkWTj",
    "outputId": "32678a3b-1150-4867-ba91-38975cf69ef3"
   },
   "outputs": [],
   "source": [
    "# prompt: Save the .keras and .h5 file to my google drive\n",
    "\n",
    "# Save the model to your Google Drive\n",
    "model.save('./best_model.h5')\n",
    "model.save('./best_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
