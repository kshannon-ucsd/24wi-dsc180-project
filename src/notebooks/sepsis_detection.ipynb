{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054460d-6af0-4e0f-b088-b6695e602430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install imblearn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ead1b-5b28-41a4-8d86-aa385f509762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9827411-fa02-4b8a-8737-606f465b845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bucket and file names\n",
    "bucket_name = \"sagemaker-capstone-bucket\"\n",
    "subset_file = \"train/second_model_subset.csv\"\n",
    "feature_file = \"train/second_features.csv\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d19750-0205-43a6-90bc-a7c18cd2c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read subset CSV\n",
    "subset_obj = s3_client.get_object(Bucket=bucket_name, Key=subset_file)\n",
    "subset = pd.read_csv(StringIO(subset_obj[\"Body\"].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5dcad-1122-44bc-acea-8036e52671d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_obj = s3_client.get_object(Bucket=bucket_name, Key=feature_file)\n",
    "pneumonia = pd.read_csv(StringIO(pneumonia_obj[\"Body\"].read().decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1407e-cf6b-49ce-9cfa-ee54bea4ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00aa1ff-dc18-45dc-817b-e0f8e1cb6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pneu = pneumonia[pneumonia['subject_id'].notna()]\n",
    "\n",
    "pneu = pneu[[\n",
    "     'subject_id',\n",
    "     'charttime',\n",
    "     'hadm_id',\n",
    "     'stay_id',\n",
    "     'heart_rate',\n",
    "     'sbp',\n",
    "     'sbp_ni',\n",
    "     'mbp',\n",
    "     'mbp_ni',\n",
    "     'resp_rate',\n",
    "     'temperature',\n",
    "     'platelet',\n",
    "     'wbc',\n",
    "     'bands',\n",
    "     'lactate',\n",
    "     'inr',\n",
    "     'ptt',\n",
    "     'creatinine',\n",
    "     'bilirubin',\n",
    "     'pneumonia'\n",
    "]]\n",
    "\n",
    "# getting most recent lab result\n",
    "recents = (pneu.sort_values(['subject_id', 'hadm_id', 'stay_id', 'charttime'])\n",
    "           .groupby(['subject_id', 'hadm_id', 'stay_id'])\n",
    "           .tail(1))\n",
    "\n",
    "recents = recents.reset_index().drop(columns='index')\n",
    "\n",
    "# use the most recent lab result as current data for each subject per admission\n",
    "means = (pneu.groupby(['subject_id', 'hadm_id', 'stay_id'])[['heart_rate',\n",
    "                                                             'sbp',\n",
    "                                                             'mbp',\n",
    "                                                             'resp_rate',\n",
    "                                                             'temperature',\n",
    "                                                             'platelet',\n",
    "                                                             'wbc',\n",
    "                                                             'bands',\n",
    "                                                             'lactate',\n",
    "                                                             'inr',\n",
    "                                                             'ptt',\n",
    "                                                             'creatinine',\n",
    "                                                             'bilirubin',\n",
    "                                                             'pneumonia']]\n",
    "         .mean()\n",
    "         .reset_index())\n",
    "feat_squeeze = recents.combine_first(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5a675-81d3-4368-8d42-e2b0cbaf60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = (subset.merge(feat_squeeze, how='left', on=['subject_id',\n",
    "                                                        'hadm_id',\n",
    "                                                        'stay_id'])\n",
    "             .get(['heart_rate',\n",
    "                   'sbp',\n",
    "                   'mbp',\n",
    "                   'resp_rate',\n",
    "                   'temperature',\n",
    "                   'platelet',\n",
    "                   'wbc',\n",
    "                   'bands',\n",
    "                   'lactate',\n",
    "                   'inr',\n",
    "                   'ptt',\n",
    "                   'creatinine',\n",
    "                   'bilirubin',\n",
    "                   'pneumonia',\n",
    "                   'sepsis3']))\n",
    "\n",
    "full_data['sepsis3'] = full_data['sepsis3'].astype(int)\n",
    "full_data = full_data.rename(columns={'sepsis3': 'sepsis'}, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fc181-0a29-4db0-9a8b-ff0b9f6f56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_imputation(df, seed=42):\n",
    "    \"\"\"\n",
    "    Impute missing values probabilistically based on the sepsis feature.\n",
    "    Imputes by sampling from observed distributions.\n",
    "    -----------\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe with missing values\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility\n",
    "    --------\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The dataframe with imputed values\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Create a copy for imputation\n",
    "    imputed_df = df_cleaned = df.copy().copy()\n",
    "\n",
    "    # Get sepsis and non-sepsis indices\n",
    "    sepsis_idx = df_cleaned['sepsis'] == 1\n",
    "    nonsepsis_idx = df_cleaned['sepsis'] == 0\n",
    "\n",
    "    for col in df_cleaned.columns:\n",
    "        if col == 'sepsis':\n",
    "            continue  # Skip the target column\n",
    "\n",
    "        if df_cleaned[col].isna().any():\n",
    "            print(f\"Imputing missing values for {col}...\")\n",
    "            # For binary features (like pneumonia)\n",
    "            if col == 'pneumonia':\n",
    "                # For sepsis patients\n",
    "                sepsis_pneumonia_values = (df_cleaned.loc[sepsis_idx, col]\n",
    "                                           .dropna())\n",
    "                if len(sepsis_pneumonia_values) > 0:\n",
    "                    # Probability of pneumonia=1\n",
    "                    sepsis_pneumonia_prob = sepsis_pneumonia_values.mean()\n",
    "                else:\n",
    "                    sepsis_pneumonia_prob = 0.5  # Default if no data\n",
    "\n",
    "                # For non-sepsis patients\n",
    "                nonsepsis_pneumonia_values = (df_cleaned.loc[nonsepsis_idx, col]\n",
    "                                              .dropna())\n",
    "                if len(nonsepsis_pneumonia_values) > 0:\n",
    "                    nonsepsis_pneumonia_prob = nonsepsis_pneumonia_values.mean()\n",
    "                else:\n",
    "                    nonsepsis_pneumonia_prob = 0.3  # Default if no data\n",
    "\n",
    "                # Impute for sepsis patients\n",
    "                missing_sepsis = sepsis_idx & df_cleaned[col].isna()\n",
    "                if missing_sepsis.any():\n",
    "                    n_missing = missing_sepsis.sum()\n",
    "                    imputed_df.loc[missing_sepsis, col] = np.random.binomial(\n",
    "                        1,\n",
    "                        sepsis_pneumonia_prob,\n",
    "                        size=n_missing)\n",
    "\n",
    "                # Impute for non-sepsis patients\n",
    "                missing_nonsepsis = nonsepsis_idx & df_cleaned[col].isna()\n",
    "                if missing_nonsepsis.any():\n",
    "                    n_missing = missing_nonsepsis.sum()\n",
    "                    imputed_df.loc[missing_nonsepsis, col] = np.random.binomial(\n",
    "                        1,\n",
    "                        nonsepsis_pneumonia_prob,\n",
    "                        size=n_missing)\n",
    "\n",
    "            # For continuous features\n",
    "            else:\n",
    "                # For sepsis group - sample from observed clean values\n",
    "                sepsis_values = df_cleaned.loc[sepsis_idx, col].dropna()\n",
    "\n",
    "                # Impute for sepsis patients\n",
    "                missing_sepsis = sepsis_idx & df_cleaned[col].isna()\n",
    "                if missing_sepsis.any():\n",
    "                    n_missing = missing_sepsis.sum()\n",
    "                    imputed_df.loc[missing_sepsis, col] = np.random.choice(\n",
    "                        sepsis_values,\n",
    "                        size=n_missing,\n",
    "                        replace=True)\n",
    "\n",
    "                # For non-sepsis group\n",
    "                nonsepsis_values = df_cleaned.loc[nonsepsis_idx, col].dropna()\n",
    "\n",
    "                # Impute for non-sepsis patients\n",
    "                missing_nonsepsis = nonsepsis_idx & df_cleaned[col].isna()\n",
    "                if missing_nonsepsis.any():\n",
    "                    n_missing = missing_nonsepsis.sum()\n",
    "                    imputed_df.loc[missing_nonsepsis, col] = np.random.choice(\n",
    "                        nonsepsis_values,\n",
    "                        size=n_missing,\n",
    "                        replace=True)\n",
    "    print(\"====================================\")\n",
    "    print(f'Total number of imputed columns: {len(df.columns) - 1}')\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3cd33-09fe-48fc-bcdb-56db1daac827",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_metadata = probabilistic_imputation(full_data, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28630534-b769-4b4e-b7bc-863779712130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compare_distributions(original_df, imputed_df, feature):\n",
    "    \"\"\"\n",
    "    Compare distributions of original vs imputed data, separated by sepsis status\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # Sepsis patients - original data\n",
    "    sns.histplot(original_df.loc[original_df['sepsis'] == 1, feature].dropna(),\n",
    "                 kde=True, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'Original {feature} - Sepsis Patients')\n",
    "\n",
    "    # Non-sepsis patients - original data\n",
    "    sns.histplot(original_df.loc[original_df['sepsis'] == 0, feature].dropna(),\n",
    "                 kde=True, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'Original {feature} - Non-sepsis Patients')\n",
    "\n",
    "    # Sepsis patients - imputed data\n",
    "    sns.histplot(imputed_df.loc[imputed_df['sepsis'] == 1, feature],\n",
    "                 kde=True, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Imputed {feature} - Sepsis Patients')\n",
    "\n",
    "    # Non-sepsis patients - imputed data\n",
    "    sns.histplot(imputed_df.loc[imputed_df['sepsis'] == 0, feature],\n",
    "                 kde=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Imputed {feature} - Non-sepsis Patients')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Usage:\n",
    "fig = compare_distributions(full_data, imputed_metadata, 'temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9818c25-8d98-4d06-89cd-8f0ff8a5a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed_metadata.drop(columns=[\"sepsis\"])\n",
    "y = imputed_metadata.sepsis\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bab92-b9b2-44f7-8a25-b31c38bce550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# CatBoost parameters\n",
    "params = {\n",
    "    'iterations': 3000,              # Similar to n_estimators in XGBoost\n",
    "    'learning_rate': 0.4,            # Step size shrinkage\n",
    "    'depth': 2,                      # Equivalent to max_depth\n",
    "    'min_data_in_leaf': 4,           # Similar to min_child_weight\n",
    "    'subsample': 0.8,                # Random subsampling of data\n",
    "    'colsample_bylevel': 0.9,        # Similar to colsample_bytree\n",
    "    'l2_leaf_reg': 0.3,              # L2 regularization\n",
    "    'random_seed': 42,               # Random state for reproducibility\n",
    "    'loss_function': 'Logloss',      # (equivalent to binary:logistic)\n",
    "    'eval_metric': 'AUC',            # Area under curve metric\n",
    "    'bootstrap_type': 'Bernoulli',   # Type of bootstrap to perform\n",
    "    'verbose': False,                # Suppress verbose output\n",
    "    'early_stopping_rounds':20\n",
    "}\n",
    "\n",
    "# Pipeline for classification\n",
    "positive_pipeline = Pipeline([\n",
    "    ('classifier', CatBoostClassifier(**params))\n",
    "])\n",
    "\n",
    "positive_pipeline.fit(X_train, y_train)\n",
    "y_train_pred = positive_pipeline.predict(X_train)\n",
    "y_test_pred = positive_pipeline.predict(X_test)\n",
    "train_report = classification_report(y_train, y_train_pred, output_dict=True)\n",
    "test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "print(\"==================\", end='\\n')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30411918-d525-4850-afe7-699d5496222f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_report).T\n",
    "test_df = pd.DataFrame(test_report).T\n",
    "\n",
    "# Add accuracy scores\n",
    "train_df.loc[\"accuracy\"] = [\"\", \"\", \"\", accuracy_score(y_train, y_train_pred)]\n",
    "test_df.loc[\"accuracy\"] = [\"\", \"\", \"\", accuracy_score(y_test, y_test_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b364ed9-c814-4112-86cd-a3d295a2dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = positive_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute FPR, TPR, and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"(AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")  # Diagonal line (random model)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig(\"roc_curve_2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.ylim([0.0, 1.0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfe072-e1ad-4f50-bc32-649617cc98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# 1. Feature Importance\n",
    "def plot_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance from a CatBoost model\n",
    "    \"\"\"\n",
    "    # Extract the CatBoost model from the pipeline\n",
    "    catboost_model = model.named_steps['classifier']\n",
    "\n",
    "    # Get feature importance\n",
    "    importances = catboost_model.get_feature_importance()\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Plot top N features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n))\n",
    "    plt.title('CatBoost Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "# 2. Permutation Importance (alternative measure that shows feature impact on performance)\n",
    "def plot_permutation_importance(model, X_test, y_test, feature_names, top_n=20, n_repeats=10):\n",
    "    \"\"\"\n",
    "    Calculate and plot permutation importance for features\n",
    "    \"\"\"\n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test, y_test, n_repeats=n_repeats, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Plot top N features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=perm_importance_df.head(top_n))\n",
    "    plt.title('Permutation Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return perm_importance_df\n",
    "\n",
    "\n",
    "# 3. Partial Dependence Plots (shows how a feature affects predictions)\n",
    "def plot_partial_dependence(model, X, feature_names, feature_idx, grid_resolution=50):\n",
    "    \"\"\"\n",
    "    Create partial dependence plot for a specific feature\n",
    "    \"\"\"\n",
    "    # Extract CatBoost model from pipeline\n",
    "    catboost_model = model.named_steps['classifier']\n",
    "\n",
    "    # Get feature values for the selected feature\n",
    "    feature_values = np.linspace(\n",
    "        np.min(X[:, feature_idx]),\n",
    "        np.max(X[:, feature_idx]),\n",
    "        num=grid_resolution\n",
    "    )\n",
    "\n",
    "    # Create a grid for prediction\n",
    "    X_pd = np.tile(X.mean(axis=0), (grid_resolution, 1))\n",
    "\n",
    "    # Replace the feature values with our grid\n",
    "    X_pd[:, feature_idx] = feature_values\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = catboost_model.predict_proba(X_pd)[:, 1]\n",
    "\n",
    "    # Plot the partial dependence\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(feature_values, predictions)\n",
    "    plt.xlabel(feature_names[feature_idx])\n",
    "    plt.ylabel('Predicted probability')\n",
    "    plt.title(f'Partial Dependence Plot: {feature_names[feature_idx]}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 4. SHAP Values (shows contribution of each feature to each prediction)\n",
    "def plot_shap_values(model, X, feature_names, sample_size=100):\n",
    "    \"\"\"\n",
    "    Calculate and plot SHAP values for CatBoost model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "\n",
    "        # Extract CatBoost model from pipeline\n",
    "        catboost_model = model.named_steps['classifier']\n",
    "\n",
    "        # Sample data for SHAP analysis if dataset is large\n",
    "        if X.shape[0] > sample_size:\n",
    "            X_sample = X.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            X_sample = X\n",
    "\n",
    "        # Create explainer\n",
    "        explainer = shap.TreeExplainer(catboost_model)\n",
    "\n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, feature_names=feature_names)\n",
    "\n",
    "        # Dependence plots for top features\n",
    "        importance = np.abs(shap_values).mean(0)\n",
    "        indices = np.argsort(importance)[-5:]  # Top 5 features\n",
    "\n",
    "        for idx in indices:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            shap.dependence_plot(idx, shap_values, X_sample, feature_names=feature_names)\n",
    "\n",
    "        return explainer, shap_values\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"SHAP package not installed. Install with: pip install shap\")\n",
    "        return None, None\n",
    "\n",
    "# 5. ROC and Precision-Recall curves\n",
    "def plot_model_curves(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot ROC and Precision-Recall curves for both training and test sets\n",
    "    \"\"\"\n",
    "    # Get probabilities\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc_train = roc_auc_score(y_train, y_train_proba)\n",
    "    auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr_train, tpr_train, label=f'Train AUC: {auc_train:.3f}')\n",
    "    plt.plot(fpr_test, tpr_test, label=f'Test AUC: {auc_test:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate Precision-Recall curve\n",
    "    precision_train, recall_train, _ = precision_recall_curve(y_train, y_train_proba)\n",
    "    precision_test, recall_test, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_train, precision_train, label='Train')\n",
    "    plt.plot(recall_test, precision_test, label='Test')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'auc_train': auc_train,\n",
    "        'auc_test': auc_test\n",
    "    }\n",
    "\n",
    "\n",
    "# 6. Model calibration plot\n",
    "def plot_calibration_curve(model, X_train, y_train, X_test, y_test, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plot calibration curve to check if predicted probabilities match observed frequencies\n",
    "    \"\"\"\n",
    "    # Get probabilities\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Function to calculate calibration curve\n",
    "    def calculate_calibration(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "        bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "        bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "        bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "        nonzero = bin_total != 0\n",
    "        prob_true = bin_true[nonzero] / bin_total[nonzero]\n",
    "        prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n",
    "\n",
    "        return prob_true, prob_pred\n",
    "\n",
    "    # Calculate calibration for train and test\n",
    "    prob_true_train, prob_pred_train = calculate_calibration(y_train, y_train_proba, n_bins)\n",
    "    prob_true_test, prob_pred_test = calculate_calibration(y_test, y_test_proba, n_bins)\n",
    "\n",
    "    # Plot calibration curve\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "    plt.plot(prob_pred_train, prob_true_train, 's-', label='Train')\n",
    "    plt.plot(prob_pred_test, prob_true_test, 's-', label='Test')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 7. Learning curves to check for overfitting\n",
    "def plot_learning_curves(model, X_train, y_train, X_test, y_test, cv=5):\n",
    "    \"\"\"\n",
    "    Plot learning curves to check for overfitting/underfitting\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import learning_curve\n",
    "\n",
    "    # Calculate learning curves\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X_train, y_train, \n",
    "        cv=cv, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy', \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Calculate mean and std\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_full_analysis(model, X_train, y_train, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Run a comprehensive model interpretation and analysis\n",
    "    \"\"\"\n",
    "    print(\"=== Model Analysis ===\")\n",
    "    print(\"1. Feature Importance\")\n",
    "    importance_df = plot_feature_importance(model, feature_names)\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    print(\"\\n2. Permutation Importance\")\n",
    "    perm_importance_df = plot_permutation_importance(model, X_test, y_test, feature_names)\n",
    "    print(perm_importance_df.head(10))\n",
    "\n",
    "    print(\"\\n3. Model Performance Curves\")\n",
    "    metrics = plot_model_curves(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"\\n4. Model Calibration\")\n",
    "    plot_calibration_curve(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"\\n5. Learning Curves\")\n",
    "    plot_learning_curves(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"\\n6. Partial Dependence Plots for top features\")\n",
    "    for idx in importance_df.head(5).index:\n",
    "        feature_idx = list(feature_names).index(importance_df.iloc[idx]['Feature'])\n",
    "        plot_partial_dependence(model, X_train.values, feature_names, feature_idx)\n",
    "\n",
    "    print(\"\\n7. SHAP Analysis\")\n",
    "    plot_shap_values(model, X_train, feature_names)\n",
    "\n",
    "    return {\n",
    "        'feature_importance': importance_df,\n",
    "        'permutation_importance': perm_importance_df,\n",
    "        'performance_metrics': metrics\n",
    "    }\n",
    "\n",
    "# Example of usage:\n",
    "run_full_analysis(positive_pipeline, X_train, y_train, X_test, y_test, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a6553-07b1-491d-a379-fc45430ef899",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_values(positive_pipeline, X_train, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a98f37-2a43-4bed-927f-f98f28d1c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open(\"best_second_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(positive_pipeline, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
